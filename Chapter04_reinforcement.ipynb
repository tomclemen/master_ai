{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "apparent-criterion",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Gym https://gym.openai.com/ is a toolkit for developing and comparing reinforcement learning algorithms. This notebook is an extented version of the ```gym``` documentation https://gym.openai.com/docs.\n",
    "\n",
    "We'll have a quick start. To get started, you’ll need to have Python 3.5+ installed. Simply install gym using pip:\n",
    "\n",
    "```python\n",
    "pip install gym\n",
    "pip install gym[atari]\n",
    "````\n",
    "\n",
    "@MacOS Big Sur users: please note that in some cases you'll need to execute the following command after installing 'gym':\n",
    "\n",
    "```python\n",
    "pip install pyglet==1.5.16\n",
    "````\n",
    "\n",
    "This is because the gym installer comes with an older pyglet version that doesn't work with Big Sur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-mining",
   "metadata": {},
   "source": [
    "Luckily, Gym comprises a set of predefined environments. Let's start with a most basic 'Hello World' example. For now, please ignore the warning about calling ```step()``` even though this environment has already returned ```done = True```.\n",
    "If you’d like to see some other environments in action, try replacing ```CartPole-v0``` with other environments. Please note that in most cases, I've needed to restart the kernel before the visualization became visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.make('MountainCar-v0')\n",
    "#env = gym.make('MsPacman-v0')\n",
    "\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-chambers",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "If we ever want to do better than take random actions at each step, it’d probably be good to actually know what our actions are doing to the environment.\n",
    "\n",
    "The environment’s ```step``` function returns exactly what we need. In fact, ```step``` returns four values. These are:\n",
    "\n",
    "* ```observation``` (**object**): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "* ```reward``` (**float**): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "* ```done``` (**boolean**): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "* ```info``` (**dict**): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "This is just an implementation of the classic “agent-environment loop”. Each timestep, the agent chooses an action, and the environment returns an observation and a reward as described in the fundamental part of this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-burden",
   "metadata": {},
   "source": [
    "The process gets started by calling ```reset()```, which returns an initial ```observation```. So a more proper way of writing the previous code would be to respect the ```done``` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-literacy",
   "metadata": {},
   "source": [
    "### Quick questions\n",
    "\n",
    "1. What are the specific differences between the first and the second code example? Why did the warning disappear?\n",
    "2. What does the ```i_episode``` variable means in the second example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-attribute",
   "metadata": {},
   "source": [
    "## Spaces\n",
    "\n",
    "In the examples above, we’ve been sampling random actions from the environment’s action space. But what actually are those actions? Every environment comes with an ```action_space``` and an ```observation_space```. These attributes are of type ```Space```, and they describe the format of valid actions and observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-robin",
   "metadata": {},
   "source": [
    "The ```Discrete``` space allows a fixed range of non-negative numbers, so in this case valid ```actions``` are either 0 or 1. The ```Box``` space represents an n-dimensional box, so valid ```observations``` will be an array of 4 numbers. We can also check the Box’s bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-retro",
   "metadata": {},
   "source": [
    "This introspection can be helpful to write generic code that works for many different environments. ```Box``` and ```Discrete``` are the most common Spaces. You can sample from a ```Space``` or check that something belongs to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "space = spaces.Discrete(8) # Set with 8 elements {0, 1, 2, ..., 7}\n",
    "x = space.sample()\n",
    "assert space.contains(x)\n",
    "assert space.n == 8\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-workshop",
   "metadata": {},
   "source": [
    "### Quick questions\n",
    "\n",
    "1. Please identify the corresponding explainations within the lecture slides.\n",
    "2. What other ```spaces```are available in ```gym```?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-google",
   "metadata": {},
   "source": [
    "## Gym's environment registry\n",
    "\n",
    "```Gym```’s main purpose is to provide a large collection of environments that expose a common interface and are versioned to allow for comparisons. To list the environments available in your installation, just ask ```gym.envs.registry```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-noise",
   "metadata": {},
   "source": [
    "This will give you a list of ```EnvSpec``` objects. These define parameters for a particular task, including the number of trials to run and the maximum number of steps. For example, ```EnvSpec(Hopper-v1)``` defines an environment where the goal is to get a 2D simulated robot to hop; ```EnvSpec(Go9x9-v0)``` defines a Go game on a 9x9 board.\n",
    "\n",
    "These environment IDs are treated as opaque strings. In order to ensure valid comparisons for the future, environments will never be changed in a fashion that affects performance, only replaced by newer versions. We currently suffix each environment with a ```v0``` so that future replacements can naturally be called ```v1```, ```v2```, etc.\n",
    "\n",
    "It’s very easy to add your own enviromments to the registry, and thus make them available for ```gym.make()```: just ```register()``` them at load time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-anthony",
   "metadata": {},
   "source": [
    "## Working with predefined environments\n",
    "\n",
    "You have probably been overwhelmed by the above list of EnvSpec objects.```Gym``` is full of exciting RL examples, and it is perhaps worth exploring them a bit more. Please feel free to run your own experiments. Try to understand what the background is and why RL might bring value to it.\n",
    "\n",
    "Please give me and the team feedback if you have found a particularly worthwhile environment. Enjoy the experiments!\n",
    "\n",
    "If you are more interested in diving into the RL algorithms, you'll find another notebook also within the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-spank",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
